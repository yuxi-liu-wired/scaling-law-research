{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07b75c28-6d6e-4de9-9851-564048cc0207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1390c5ef-721b-4e7b-a1de-b71c9b41b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(98958)\n",
      "tensor([ 98958, 111097, 205484])\n",
      "tensor([ 98958, 193427])\n",
      "tensor(104222)\n"
     ]
    }
   ],
   "source": [
    "for model_id in ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']:\n",
    "    intervals = torch.load(f'wiki_arithmetic_code_{model_id}_test_1.pt')\n",
    "    print(torch.nonzero(intervals[:,1] - intervals[:,0] <= 0).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fc436a6-fb08-4ae0-8edc-5b3f911160b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing wiki_arithmetic_code_gpt2_test_1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (258499 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length = 258499\n",
      "\n",
      " lavishly praised for their performances and the film is summed up as not to be missed. paul newman reprised his role as fast eddie felson in the film the color of money, for which he won the academy award for best actor in a leading role. a number of observers and critics have suggested that this oscar was in belated recognition for his performance in the hustler. in, the library of congress selected the hustler for preservation in the united states national film registry as culturally, historically, or aesthetically significant. carroll and rossen s screenplay was selected by the writers guild of america in as the th best motion picture screenplay of all time. in june, afi released its ten top ten the best ten films in ten classic american film genres after polling over, people from the creative community. the hustler was acknowledged as the sixth best film in the sports genre. the hustler is credited with sparking a resurgence in the popularity of pool in the united states, which had been on the decline for decades. the film also brought recognition to willie mosconi, who, despite having won multiple world championships, was virtually unknown to the general public. perhaps the greatest beneficiary of the film s popularity was a real life pool hustler named rudolf wanderone. mosconi claimed in an interview at the time of the film s release that the character of minnesota fats was based on wanderone, who at the time was known as new york fatty. wanderone immediately adopted the minnesota fats nickname and parlayed his association with the film into book and television deals and other ventures. author walter tevis denied for the rest of his life that wanderone had played any role in the creation of the character. other players would claim, with greater or lesser degrees of credibility, to have served as models for fast eddie, including ronnie allen, ed taylor, ed parker, and eddie pelkey. \n"
     ]
    }
   ],
   "source": [
    "model_id = 'gpt2'\n",
    "dataset_split = 'test'\n",
    "stride = 1\n",
    "print(f\"Computing wiki_arithmetic_code_{model_id}_{dataset_split}_{stride}...\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Load the model\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "device = \"cpu\"\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Load the dataset\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "allowed_chars = set(\"abcdefghijklmnopqrstuvwxyz .,\")\n",
    "\n",
    "dataset_name = \"wikitext-103-truncated\"\n",
    "dataset = {}\n",
    "for item in [\"valid\", \"test\"]:\n",
    "    with open(\n",
    "        f\"{dataset_name}/wiki_shannonfied.{dataset_split}.txt\", \"r\", encoding=\"utf-8\"\n",
    "    ) as f:\n",
    "        dataset[dataset_split] = f.read()\n",
    "\n",
    "encodings = tokenizer(dataset[dataset_split], return_tensors=\"pt\")\n",
    "encodings.input_ids = encodings.input_ids.to(device)\n",
    "\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "print(f\"Sequence length = {seq_len}\\n\")\n",
    "print(tokenizer.decode(encodings.input_ids[0][seq_len-400:]))\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Calculate intervals\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def logit_to_cdf(logit):\n",
    "    prob = np.exp(logit - logit.max())\n",
    "    cdf = np.cumsum(prob)\n",
    "    cdf /= cdf.max()\n",
    "    cdf = np.concatenate((np.zeros(1), cdf))\n",
    "    return cdf\n",
    "\n",
    "def logit_array_to_cdf(logit, axis, epsilon=1e-9):\n",
    "    if isinstance(logit, np.ndarray):\n",
    "        logit = torch.tensor(logit)\n",
    "\n",
    "    max_logit = torch.max(logit, axis=axis, keepdims=True)[0]\n",
    "    prob = torch.exp(logit - max_logit)\n",
    "    prob /= prob.sum(axis=axis, keepdims=True)\n",
    "    prob += epsilon\n",
    "    cdf = torch.cumsum(prob, axis=axis)\n",
    "    cdf /= torch.max(cdf, axis=axis, keepdims=True)[0]\n",
    "    # append 0 to the beginning of the cdf along axis=axis\n",
    "    shape = list(cdf.shape)\n",
    "    shape[axis] = 1\n",
    "    cdf = torch.concatenate((torch.zeros(shape).to(cdf.device), cdf), axis=axis)\n",
    "    return cdf\n",
    "\n",
    "def get_intervals(logits, symbols, epsilon=1e-9):\n",
    "    original_shape = logits.shape\n",
    "    logits = logits.reshape(-1, logits.shape[-1])\n",
    "    symbols = symbols.reshape(-1)\n",
    "\n",
    "    cdf = logit_array_to_cdf(logits, axis=1, epsilon=epsilon)\n",
    "\n",
    "    intervals = []\n",
    "    for i, symbol in enumerate(symbols):\n",
    "        lower_bound = cdf[i, symbol]\n",
    "        upper_bound = cdf[i, symbol + 1]\n",
    "\n",
    "        intervals.append([lower_bound.item(), upper_bound.item()])\n",
    "\n",
    "    # Convert intervals list to a tensor\n",
    "    intervals_tensor = torch.tensor(intervals)\n",
    "    intervals_tensor = intervals_tensor.reshape(original_shape[:-1] + (2,))\n",
    "    return intervals_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f6b4f9c7-255e-4442-a182-f3f9bd00d03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27870]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "max_length = model.config.n_positions\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "intervals_list = []\n",
    "bad_index = 98958\n",
    "for begin_loc in tqdm(range(bad_index, bad_index+1, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    if end_loc + 1 >= seq_len:\n",
    "        break # Let's just throw away the tail. It's easier than dealing with the annoying tail.\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    logits = outputs.logits[0, -stride:]\n",
    "    symbols = encodings.input_ids[:, end_loc+1-stride:end_loc+1]\n",
    "    print(symbols)\n",
    "    intervals_list.append(get_intervals(logits, symbols, epsilon=1e-7))\n",
    "\n",
    "bad_intervals = torch.cat(intervals_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f51bee1-1a74-4906-9e09-a8d9a0fefb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-237.1587, -234.2314, -237.7939,  ..., -249.6300, -248.3404,\n",
      "         -235.4836]])\n",
      "tensor([284])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(logits)\n",
    "print(encodings.input_ids[:, bad_index])\n",
    "print(logits.exp()[0,284])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6512d98-f9de-4c4a-822a-91b2eaa9e29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6785e-04, -4.5776e-05,  1.5259e-05,  ..., -6.1035e-05,\n",
      "         -7.6294e-05,  1.2207e-04]])\n",
      "tensor(1.1921e-07)\n",
      "tensor(5.3854e-07)\n",
      "tensor(5.3842e-07)\n"
     ]
    }
   ],
   "source": [
    "bugged_logits = torch.load('debug_gpt2_test_1.pt', map_location=torch.device('cpu'))\n",
    "print(logits - bugged_logits)\n",
    "epsilon = 1e-7\n",
    "\n",
    "cdf = logit_array_to_cdf(logits, axis=1, epsilon=epsilon)\n",
    "bugged_cdf = logit_array_to_cdf(bugged_logits, axis=1, epsilon=epsilon)\n",
    "print((cdf - bugged_cdf).max())\n",
    "bad_id = 284\n",
    "print(cdf[0,bad_id+1] - cdf[0,bad_id])\n",
    "print(bugged_cdf[0,bad_id+1] - bugged_cdf[0,bad_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3990fa61-cde5-41a5-931d-18c6513974ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1921e-07)\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-7\n",
    "print(get_intervals(bugged_logits, torch.tensor([27870]), epsilon)[0,1] - get_intervals(bugged_logits, torch.tensor([27870]), epsilon)[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7127a0cf-7279-47fa-8f79-f6475e023b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n",
      "tensor([ 5.6726e-04,  8.4885e-01,  5.4737e-02, -1.1921e-07,  3.8313e-01,\n",
      "         2.3260e-01])\n",
      "tensor([[-2.9802e-07, -3.5763e-07],\n",
      "        [ 5.1782e-07, -1.3113e-06],\n",
      "        [ 4.6492e-06,  2.5630e-06],\n",
      "        [ 0.0000e+00, -2.3842e-07],\n",
      "        [ 1.4007e-06, -1.7881e-06],\n",
      "        [-8.9034e-07, -2.2948e-06]])\n",
      "tensor([ 5.1636e-04,  8.6871e-01,  3.9888e-02, -1.1921e-07,  1.3010e-02,\n",
      "         2.0468e-01])\n",
      "tensor([[-0.1110, -0.1110],\n",
      "        [-0.0063,  0.0135],\n",
      "        [-0.0438, -0.0587],\n",
      "        [-0.0007, -0.0007],\n",
      "        [ 0.1695, -0.2006],\n",
      "        [-0.0101, -0.0380]])\n",
      "tensor([ 4.5690e-04,  9.4215e-01,  2.0052e-03, -1.1921e-07,  3.8215e-01,\n",
      "         4.1126e-01])\n",
      "tensor([[-0.1529, -0.1530],\n",
      "        [-0.0108,  0.0825],\n",
      "        [-0.0433, -0.0960],\n",
      "        [-0.0014, -0.0014],\n",
      "        [-0.0290, -0.0299],\n",
      "        [-0.0258,  0.1529]])\n",
      "tensor([1.1526e-03, 9.6889e-01, 4.2533e-03, 4.1723e-07, 2.1789e-01, 2.6212e-01])\n",
      "tensor([[-0.0395, -0.0389],\n",
      "        [-0.0226,  0.0974],\n",
      "        [ 0.0866,  0.0361],\n",
      "        [-0.0083, -0.0083],\n",
      "        [ 0.0435, -0.1217],\n",
      "        [-0.0222,  0.0073]])\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "for model_id in ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']:\n",
    "    intervals = torch.load(f'wiki_arithmetic_code_{model_id}_test_1.pt')\n",
    "    print(intervals[bad_index-3:bad_index+3,1] - intervals[bad_index-3:bad_index+3,0])\n",
    "    print(intervals[bad_index-3:bad_index+3] - bad_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bd74c7-f016-4d9b-a0ee-bb0d27e53036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
